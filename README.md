# LangChain RAG Application with FastAPI and Streamlit

A production-ready Question-Answering system built using **Retrieval-Augmented Generation (RAG)**, combining **Large Language Models (LLMs)** with **semantic search** for intelligent document analysis.

---

## ğŸš€ Key Features

- ğŸ” **Retrieval-Augmented Generation**: Combines document retrieval with LLM response generation for context-aware answers.
- ğŸ“„ **Multi-Format Document Support**: Upload and process PDF, DOCX, and HTML files.
- ğŸ’¬ **Session-Based Chat Memory**: Maintains conversation history across interactions.
- ğŸ§  **Chroma Vector DB Integration**: Efficient semantic search powered by ChromaDB.
- âš¡ **FastAPI Backend**: RESTful APIs with automatic documentation.
- ğŸ–¥ï¸ **Streamlit Frontend**: Simple and responsive chatbot UI.
- ğŸ—ƒï¸ **Persistent Storage**: Uses SQLite to store metadata and chat history.

---

## ğŸ¥ Demo

![LangChain RAG App Demo](./demo/langchain-rag-app.gif)

**[â–¶ï¸ Watch Full Demo Video](./demo/langchain-rag-app-demo.mp4)**

The demo showcases:
- Document upload (PDF/DOCX/HTML)
- Real-time question answering with context retrieval
- Session-based conversation memory
- Document management (list/delete)

---

## ğŸ—ï¸ Architecture Overview

```
[ User ] 
   â†“
Streamlit Chat UI 
   â†“
FastAPI Backend (LangChain LCEL Chains)
   â†“
ChromaDB (Vector Store) + SQLite (Metadata)
   â†“
OpenAI (LLM Inference)
```

---

## ğŸ§° Tech Stack

- **LangChain LCEL** â€“ Composable chain building for LLM orchestration
- **OpenAI** â€“ Large Language Models for response generation
- **FastAPI** â€“ Modern async Python web framework
- **Streamlit** â€“ UI for interactive chat experience
- **ChromaDB** â€“ Open-source vector database
- **SQLite** â€“ Lightweight database for persistence
- **Python 3.12+**

---

## ğŸ“¦ Prerequisites

- Python 3.12 or higher
- OpenAI API key
- Minimum 4GB RAM (8GB recommended)

---

## âš™ï¸ Installation

1. **Clone the Repository**
   ```bash
   git clone <repository-url>
   cd langchain-rag-app
   ```

2. **Create and Activate Virtual Environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Add Environment Variables**
   Create a `.env` file in the `be-app-fastAPI/` directory:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   ```

---

## ğŸš€ Quick Start

### Start Backend (FastAPI)
```bash
cd be-app-fastAPI
uvicorn main:app --reload --port 8000
```
Visit API docs at: [http://localhost:8000/docs](http://localhost:8000/docs)

### Start Frontend (Streamlit)
In a new terminal:
```bash
cd ui-app
streamlit run app.py
```
Access UI at: [http://localhost:8501](http://localhost:8501)

---

## ğŸ”Œ API Endpoints

| Endpoint       | Method | Description                                        |
|----------------|--------|----------------------------------------------------|
| `/chat`        | POST   | Query documents using RAG pipeline                |
| `/upload-doc`  | POST   | Upload and index documents                        |
| `/list-docs`   | GET    | List all uploaded documents with metadata         |
| `/delete-doc`  | POST   | Delete a document from vector DB and metadata DB  |

### ğŸ“˜ Example API Request

```json
POST /chat
{
  "question": "What is machine learning?",
  "session_id": "abc123",
  "model": "gpt-4o-mini"
}
```

---

## ğŸ“ Project Structure

```
langchain-rag-app/
â”œâ”€â”€ be-app-fastAPI/              # Backend - FastAPI app
â”‚   â”œâ”€â”€ main.py                  # Entry point
â”‚   â”œâ”€â”€ langchain_utils.py       # LangChain chain config
â”‚   â”œâ”€â”€ chroma_utils.py          # ChromaDB operations
â”‚   â”œâ”€â”€ db_utils.py              # SQLite helpers
â”‚   â”œâ”€â”€ pydantic_models.py       # Request/response schemas
â”‚   â””â”€â”€ chroma_db/               # Vector DB storage
â”œâ”€â”€ ui-app/                      # Frontend - Streamlit UI
â”‚   â”œâ”€â”€ app.py                   # Main UI script
â”‚   â”œâ”€â”€ chat_interface.py        # Chat logic
â”‚   â”œâ”€â”€ sidebar.py               # Upload/list interface
â”‚   â””â”€â”€ api_utils.py             # FastAPI communication
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # You're here!
```

---

## ğŸ§  How RAG Works

1. ğŸ“„ **Document Upload**: User uploads PDF, DOCX, or HTML files
2. ğŸ§¬ **Vectorization**: Content is chunked and embedded using OpenAI
3. ğŸ” **Semantic Search**: ChromaDB retrieves top relevant chunks
4. ğŸ§  **LLM Response**: OpenAI GPT generates answers using context
5. ğŸ§¾ **Session Memory**: SQLite stores Q&A history per session

---

## ğŸ’¼ Use Cases

- ğŸ’¡ Internal knowledge base for enterprises
- ğŸ¤– Customer support assistants
- âš–ï¸ Legal or contract analysis tools
- ğŸ“š Academic paper search and summarization
- ğŸ§° Interactive product documentation tools

---

Built using modern NLP, LLMs, and vector search techniques.  
Ideal for developers exploring **LangChain**, **RAG**, and **full-stack AI apps**.

---

## ğŸ”— Related Tags

`LangChain`, `RAG`, `LLM`, `FastAPI`, `Streamlit`, `ChromaDB`, `OpenAI`, `Python AI Project`, `AI Chatbot`, `Document Search`, `Semantic Search`, `Vector Store`, `GPT-4`
